{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy based Logistic Regression on Images classification\n",
    "\n",
    "    - This is a custom implementation of Logistic Regression using Numpy \n",
    "    - This implements a numpy based forward and back propogation on cats and dogs data set\n",
    "    - Cool implementation and gives a good understanding of how to implement back propogation using chain rule.\n",
    "    - This example has been developed using the cats & dogs data with 400 training images and 100 testing images\n",
    "    - Overall the model gives an accuracy of 58% :( on testing dataset but goes a long way in explaining how a basic neural network works with forward and back propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image data setup & download\n",
    "\n",
    "    - Once we have the data downloaded we will split the data into 3 folders\n",
    "        - Training\n",
    "        - Validation\n",
    "        - Testing\n",
    "        \n",
    "    - You could do them manually OR via a program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in dogs training images directory are  200\n",
      "Number of files in cats training images directory are  200\n",
      "Number of files in dogs validation directory are  100\n",
      "Number of files in cats validation directory are  100\n",
      "Number of files in dogs test directory are  50\n",
      "Number of files in cats test directory are  50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "base_dir = '.\\dataset'\n",
    "\n",
    "train_dir = os.path.join(base_dir,'training_set') \n",
    "train_dogs_dir = os.path.join(train_dir,'dogs') \n",
    "train_cats_dir = os.path.join(train_dir,'cats') \n",
    "\n",
    "validation_dir = os.path.join(base_dir,'validation_set') \n",
    "validation_dogs_dir = os.path.join(validation_dir,'dogs') \n",
    "validation_cats_dir = os.path.join(validation_dir,'cats') \n",
    "\n",
    "test_dir = os.path.join(base_dir,'test_set') \n",
    "test_dogs_dir = os.path.join(test_dir,'dogs') \n",
    "test_cats_dir = os.path.join(test_dir,'cats') \n",
    "\n",
    "test_dir = os.path.join(base_dir,'test')\n",
    "\n",
    "#print files count in each dire\n",
    "print ('Number of files in dogs training images directory are ',len(os.listdir(train_dogs_dir)))\n",
    "print ('Number of files in cats training images directory are ',len(os.listdir(train_cats_dir)))\n",
    "\n",
    "print ('Number of files in dogs validation directory are ',len(os.listdir(validation_dogs_dir)))\n",
    "print ('Number of files in cats validation directory are ',len(os.listdir(validation_cats_dir)))\n",
    "\n",
    "print ('Number of files in dogs test directory are ',len(os.listdir(test_dogs_dir)))\n",
    "print ('Number of files in cats test directory are ',len(os.listdir(test_cats_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images data pre-processing\n",
    "\n",
    "    - Once downloaded & split into folders we have the images as JPEG files. However before we can feed them into the network we need to convert them into right sized tensors; rough steps we would follow are\n",
    "    \n",
    "        - load jpeg files from disk\n",
    "        - decode jpeg files to pixels grid\n",
    "        - convert pixels grid to floating point tensors\n",
    "        - rescale the pixel values (between 0 and 255) to values (between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Let's load a sample image and apply this image generator on it.\n",
    "import os\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def load_images(images, path):\n",
    "\n",
    "    \"\"\"\n",
    "    Loads the images from the specified path in a target size of 100*100px. Function then converts the images into an array and flattens the array to a shape of (150*150*3)\n",
    "    \n",
    "    Arguments:\n",
    "    images - list to which all array representation of each loaded image is added\n",
    "    path - location from where the images files are loaded\n",
    "    \n",
    "    Return:\n",
    "    images - list of array representation of each loaded image found in the specified path\n",
    "    \"\"\"\n",
    "    #get all filenames from the specified path\n",
    "    fileNames = [os.path.join(path,fname) for fname in os.listdir(path)]\n",
    "\n",
    "    for i in range (len(fileNames)):\n",
    "\n",
    "        #select one image name for augmentation\n",
    "        sample_image_path = fileNames[i] \n",
    "\n",
    "        #load sample image\n",
    "        sample_image = image.load_img(sample_image_path, target_size=(100,100))\n",
    "\n",
    "        #conver the image to it's array representation\n",
    "        sample_image = image.img_to_array(sample_image)\n",
    "        #print ('Shape of each image before adding to list is ', sample_image.shape)\n",
    "        \n",
    "        images.append(sample_image)\n",
    "        \n",
    "    return images, len(fileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def initialize():\n",
    "    \n",
    "    trainImages = []    \n",
    "    \n",
    "    trainImages, numberOfDogImages = load_images(trainImages, train_dogs_dir)\n",
    "    dogLabels = np.zeros((numberOfDogImages,1))\n",
    "    \n",
    "    trainImages, numberOfCatImages = load_images(trainImages, train_cats_dir)\n",
    "    catLabels = np.ones((numberOfCatImages,1))\n",
    "    \n",
    "    trainImages = np.array(trainImages)\n",
    "    trainLabels = np.concatenate((dogLabels, catLabels),axis=0)\n",
    "    trainLabels = np.array(trainLabels)\n",
    "    \n",
    "    testImages = []\n",
    "    \n",
    "    testImages, numberOfDogImages = load_images(testImages, test_dogs_dir)\n",
    "    dogLabels = np.zeros((numberOfDogImages,1))\n",
    "    \n",
    "    testImages, numberOfCatImages = load_images(testImages, test_cats_dir)\n",
    "    catLabels = np.ones((numberOfCatImages,1))\n",
    "    \n",
    "    testImages = np.array(testImages)\n",
    "    testLabels = np.concatenate((dogLabels, catLabels),axis=0)\n",
    "    testLabels = np.array(testLabels)\n",
    "    \n",
    "    num_training_examples = trainImages.shape[0]\n",
    "    \n",
    "    return trainImages,testImages, trainLabels, testLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propogate(trainX, trainY, W, B, learning_rate=0.01, epochs=1):\n",
    "    \n",
    "    for i in range(0,epochs):\n",
    "        \n",
    "        #Forward propogation Starts\n",
    "        Z = np.dot(trainX, W)+B\n",
    "        activation = 1/(1+np.exp(-Z))\n",
    "        #print ('YHat shape is ', yhat.shape)\n",
    "        #Forward propogation Ends\n",
    "        \n",
    "        #Back propogation Starts\n",
    "        num_training_examples = trainX.shape[0]\n",
    "\n",
    "        #print ('trainY shape is ', trainY.shape)\n",
    "        # Find derivative of trainY w.r.t z\n",
    "        dJZ = activation - trainY\n",
    "        \n",
    "        #print ('Shape of dy is ', dy.shape)\n",
    "        \n",
    "        # Find derivative of z w.r.t w\n",
    "        dJW = np.dot(trainX.T, dJZ)/num_training_examples\n",
    "        #dJW = dJW.T\n",
    "        \n",
    "        # Find derivative of z w.r.t w\n",
    "        dJB = np.sum(dJZ)/num_training_examples\n",
    "        \n",
    "        # Update w & b\n",
    "        W = W - learning_rate*dJW\n",
    "        B = B - learning_rate*dJB\n",
    "\n",
    "    return W,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initilize_coeffecients(features):\n",
    "    b = 0.1\n",
    "    W = np.zeros((features,1))\n",
    "    print ('Shape of W is ', W.shape)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def predict(images, W, B):\n",
    "    z = np.dot(images,W)+B\n",
    "    activation = 1/(1+np.exp(-z))\n",
    "    \n",
    "    #yhat = yhat.reshape(images.shape[0],1)\n",
    "    activation[activation > .5] = 1\n",
    "    activation[activation <= .5] = 0\n",
    "        \n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions ends - Invocation Starts here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialize training & testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images is 400\n",
      "Number of testing images is  100\n",
      "Shape of train images is  (400, 100, 100, 3)\n",
      "Shape of train labels is  (400, 1)\n",
      "Shape of test images is  (100, 100, 100, 3)\n",
      "Shape of test labels is  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data\n",
    "trainImages,testImages, trainLabels, testLabels = initialize()\n",
    "\n",
    "print ('Number of training images is', trainImages.shape[0])\n",
    "print ('Number of testing images is ', testImages.shape[0])\n",
    "print ('Shape of train images is ', trainImages.shape)\n",
    "print ('Shape of train labels is ', trainLabels.shape)\n",
    "print ('Shape of test images is ', testImages.shape)\n",
    "print ('Shape of test labels is ', testLabels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Flatten the training & testing data from rank 4 to 2\n",
    "\n",
    "    - Currently our training images dataset has a shape of (200,100,100,3). \n",
    "    - This means that there are 200 images in training data set where each image has a shape of (100,100,3)\n",
    "    - Shape of (100,100,3) means that that image has a heigh=weidth=100 and it's a colored image with 3 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images is 30000\n",
      "Number of testing images is  30000\n",
      "Shape of train images is  (400, 30000)\n",
      "Shape of train labels is  (400, 1)\n",
      "Shape of test images is  (100, 30000)\n",
      "Shape of test labels is  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "trainImages_flatten = trainImages.reshape(trainImages.shape[1]*trainImages.shape[2]*trainImages.shape[3],trainImages.shape[0])\n",
    "testImages_flatten = testImages.reshape(testImages.shape[1]*testImages.shape[2]*testImages.shape[3],testImages.shape[0])\n",
    "\n",
    "trainImages_flatten = trainImages_flatten.T\n",
    "testImages_flatten = testImages_flatten.T\n",
    "\n",
    "print ('Number of training images is', trainImages_flatten.shape[1])\n",
    "print ('Number of testing images is ', testImages_flatten.shape[1])\n",
    "print ('Shape of train images is ', trainImages_flatten.shape)\n",
    "print ('Shape of train labels is ', trainLabels.shape)\n",
    "print ('Shape of test images is ', testImages_flatten.shape)\n",
    "print ('Shape of test labels is ', testLabels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Standardize the training & test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize our dataset\n",
    "trainImages_flatten = trainImages_flatten/255\n",
    "testImages_flatten = testImages_flatten/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Shuffle the dataset (both the images and the labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "trainImages_flatten,trainLabels = shuffle(trainImages_flatten, trainLabels, random_state=0)\n",
    "testImages_flatten,testLabels = shuffle(testImages_flatten, testLabels, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. # Train the model with different epochs and then plot to see which epoch gives best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W is  (30000, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5116c584c145>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Find the weights & intercepts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpropogate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainImages_flatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Shape of W & B is %s %s'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-6d9845d8c9c0>\u001b[0m in \u001b[0;36mpropogate\u001b[1;34m(trainX, trainY, W, B, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# Find derivative of z w.r.t w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mdJW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdJZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum_training_examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m#dJW = dJW.T\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import datetime\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "epochs = [4000]\n",
    "\n",
    "for i, epoc in enumerate(epochs):\n",
    "\n",
    "    # Initialize weights at the start of each epoch\n",
    "    features = trainImages_flatten.shape[1]\n",
    "    W, B = initilize_coeffecients(features)\n",
    "\n",
    "    # get start time for this epoch\n",
    "    tic = datetime.datetime.now()\n",
    "    \n",
    "    # Find the weights & intercepts\n",
    "    W, B = propogate(trainImages_flatten, trainLabels, W, B, learning_rate=0.01,epochs=epoc)\n",
    "    \n",
    "    print ('Shape of W & B is %s %s'%(W.shape, B.shape))\n",
    "    # Check accuracy on training data by making a prediction on training images - should come close to 100%\n",
    "    predictedTrainingLabel = predict(trainImages_flatten, W, B)\n",
    "    trainining_accuracy = accuracy_score(trainLabels, predictedTrainingLabel)\n",
    "\n",
    "    # Check accuracy on testing data by making a prediction on testing images - this should be a good %\n",
    "    predictedYOnTestData = predict(testImages_flatten, W, B)\n",
    "    testing_accuracy = accuracy_score(testLabels, predictedYOnTestData)\n",
    "    \n",
    "    # Append the training & testing accuracy for future plotting\n",
    "    train_accuracy.append(trainining_accuracy)\n",
    "    test_accuracy.append(testing_accuracy)\n",
    "    \n",
    "    # get end time for this epoch\n",
    "    toc = datetime.datetime.now()\n",
    "    \n",
    "    print ('Training completed for %d epocs in %s seconds'%(epoc,(toc-tic)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw accuracy graph for different Epoch values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "figure = plt.figure()\n",
    "\n",
    "ax1 = figure.add_subplot(211)\n",
    "plt.scatter(epochs, train_accuracy)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(labels=['Training'])\n",
    "\n",
    "ax1 = figure.add_subplot(212)\n",
    "plt.scatter(epochs, test_accuracy)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(labels=['Validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "    - We see that the model predicts with an accuracy of around 58% on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55]\n"
     ]
    }
   ],
   "source": [
    "print (test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
